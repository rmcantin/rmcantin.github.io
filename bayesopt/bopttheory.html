<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.9.1"/>
<title>BayesOpt: Bayesian optimization</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">BayesOpt
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.9.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('bopttheory.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Bayesian optimization </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#introbopt">Introduction to Bayesian Optimization</a></li>
<li class="level1"><a href="#modbopt">Bayesian optimization general model</a></li>
<li class="level1"><a href="#modelopt">Models and functions</a><ul><li class="level2"><a href="#surrmod">Surrogate models</a></li>
<li class="level2"><a href="#kermod">Kernel (covariance) models</a><ul><li class="level3"><a href="#singker">Atomic kernels</a></li>
<li class="level3"><a href="#combker">Binary kernels</a></li>
</ul>
</li>
<li class="level2"><a href="#parmod">Parametric (mean) functions</a></li>
<li class="level2"><a href="#critmod">Selection criteria</a><ul><li class="level3"><a href="#atomcri">Atomic criteria</a></li>
<li class="level3"><a href="#combcri">Combined criteria</a></li>
</ul>
</li>
<li class="level2"><a href="#learnmod">Methods for learning the kernel parameters</a></li>
<li class="level2"><a href="#initdes">Initial design methods</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="introbopt"></a>
Introduction to Bayesian Optimization</h1>
<p>Many problems in engineering, computer science, economics, etc., require to find the extremum of a real valued function. These functions are typically continuous and sometimes smooth (e.g.: Lipschitz continuous). However, those functions do not have a closed-form expression or might be multimodal, where some of the local extrema might have a bad outcome compared to the global extremum. The evaluation of those functions might be costly.</p>
<p>Global optimization is a special case of non-convex optimization where we want to find the global extremum of a real valued function, that is, the target function. The search is done by some pointwise evaluation of the target function.</p>
<p>The objective of a global optimization algorithm is to find the sequence of points </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ x_n \in \mathcal{A} \subset \mathbb{R}^m , \;\;\; n = 1,2,\ldots \]" src="form_14.png"/>
</p>
<p> which converges to the point <img class="formulaInl" alt="$x^*$" src="form_15.png"/>, that is, the extremum of the target function, when <img class="formulaInl" alt="$n$" src="form_16.png"/> is large. The algorithm should be able to find that sequence at least for all functions from a given family.</p>
<p>As explained in <a class="el" href="citelist.html#CITEREF_Mockus94">[18]</a>, this search procedure is a sequential decision making problem where point at step <img class="formulaInl" alt="$n+1$" src="form_17.png"/> is based on decision <img class="formulaInl" alt="$d_n$" src="form_18.png"/> which considers all previous data: </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ x_{n+1} = d_n(x_{1:n},y_{1:n}) \]" src="form_19.png"/>
</p>
<p> where <img class="formulaInl" alt="$y_i = f(x_i) + \epsilon_i$" src="form_20.png"/>. For simplicity, many works assume <img class="formulaInl" alt="$\epsilon_i = 0$" src="form_21.png"/>, that is, function evaluations are deterministic. However, we can easily extend the description to include stochastic functions (e.g.: homoscedastic noise <img class="formulaInl" alt="$\epsilon_i \sim \mathcal{N}(0,\sigma)$" src="form_22.png"/>).</p>
<p>The search method is the sequence of decisions <img class="formulaInl" alt="$d = {d_0,\ldots, d_{n-1}}$" src="form_23.png"/>, which leads to the final decision <img class="formulaInl" alt="$x_{n} = x_{n}(d)$" src="form_24.png"/>. In most applications, the objective is to optimize the response of the final decisions. Then, the criteria relies on the <em>optimality</em> <em>error</em> or <em>optimality</em> <em>gap</em>, which can be expressed as: </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ \delta_n(f,d) = f\left(x_n\right) - f(x^*) \]" src="form_25.png"/>
</p>
<p> In other applications, the objective may require to converge to <img class="formulaInl" alt="$x^*$" src="form_15.png"/> in the input space. Then, we can use for example the <em>Euclidean distance error</em>: </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ \delta_n(f,d) = \|x_n - x^*\|_2 \label{eq:dist-error} \]" src="form_26.png"/>
</p>
<p> The previous equations can also be interpreted as variants of the <em>loss</em> function for the decision at each step. Thus, the optimal decision is defined as the function that minimizes the loss function: </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ d_n = \arg \min_d \delta_n(f,d) \]" src="form_27.png"/>
</p>
<p> This requires full knowledge of function <img class="formulaInl" alt="$f$" src="form_28.png"/>, which is unavailable. Instead, let assume that the target function <img class="formulaInl" alt="$f = f(x)$" src="form_29.png"/> belongs to a family of functions <img class="formulaInl" alt="$f \in F$" src="form_30.png"/>, e.g.: continuous functions in <img class="formulaInl" alt="$\mathbb{R}^m$" src="form_31.png"/>. Let also assume that the function can be represented as sample from a probability distribution over functions <img class="formulaInl" alt="$f \sim P(f)$" src="form_32.png"/>. Then, the best response case analysis for the search process is defined as the decision that optimizes the expectation of the loss function: </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ d^{BR}_n = \arg \min_d \mathbb{E}_{P(f)} \left[ \delta_n(f,d)\right]= \arg \min_d \int_F \delta_n(f,d) \; dP(f) \]" src="form_33.png"/>
</p>
<p> where <img class="formulaInl" alt="$P$" src="form_34.png"/> is a prior distribution over functions.</p>
<p>However, we can improve the equation considering that, at decision <img class="formulaInl" alt="$d_n$" src="form_18.png"/> we have already <em>observed</em> the actual response of the function at <img class="formulaInl" alt="$n-1$" src="form_35.png"/> points, <img class="formulaInl" alt="$\{x_{1:n-1},y_{1:n-1}\}$" src="form_36.png"/>. Thus, the prior information of the function can be updated with the observations and the Bayes rule: </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ P(f|x_{1:n-1},y_{1:n-1}) = \frac{P(x_{1:n-1},y_{1:n-1}|f) P(f)}{P(x_{1:n-1},y_{1:n-1})} \]" src="form_37.png"/>
</p>
<p> In fact, we can actually rewrite the equation to represent the updates sequentially: </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ P(f|x_{1:i},y_{1:i}) = \frac{P(x_{i},y_{i}|f) P(f|x_{1:i-1},y_{1:i-1})}{P(x_{i},y_{i})}, \qquad \forall \; i=1 \ldots n-1 \]" src="form_38.png"/>
</p>
<p> Thus, the previous equation can be rewritten as: </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ d^{BO}_n = \arg \min_d \mathbb{E}_{P(f|x_{1:n-1},y_{1:n-1})} \left[ \delta_n(f,d)\right] = \arg \min_d \int_F \delta_n(f,d) \; dP(f|x_{1:n-1},y_{1:n-1}) \]" src="form_39.png"/>
</p>
<p> This equation is the root of <em>Bayesian optimization</em>, where the Bayesian part comes from the fact that we are computing the expectation with respect to the posterior distribution, also called <em>belief</em>, over functions. Therefore, Bayesian optimization is a memory-based optimization algorithm.</p>
<p>As commented before, most of the theory of Bayesian optimization is related to deterministic functions, we consider also stochastic functions, that is, we assume there might be a random error in the function output. In fact, evaluations can produce different outputs if repeated. In that case, the target function is the expected output. Furthermore, in a recent paper by <a class="el" href="citelist.html#CITEREF_Gramacy2012">[5]</a> it has been shown that, even for deterministic functions, it is better to assume certain error in the observation. The main reason being that, in practice, there might be some mismodelling errors which can lead to instability of the recursion if neglected.</p>
<h1><a class="anchor" id="modbopt"></a>
Bayesian optimization general model</h1>
<p>In order to simplify the description, we are going to use a special case of Bayesian optimization model defined previously which corresponds to the most common application. In subsequent Sections we will introduce some generalizations for different applications.</p>
<p>Without loss of generality, consider the problem of finding the minimum of an unknown real valued function <img class="formulaInl" alt="$f:\mathbb{X} \rightarrow \mathbb{R}$" src="form_40.png"/>, where <img class="formulaInl" alt="$\mathbb{X}$" src="form_41.png"/> is a compact space, <img class="formulaInl" alt="$\mathbb{X} \subset \mathbb{R}^d, d \geq 1$" src="form_42.png"/>. Let <img class="formulaInl" alt="$P(f)$" src="form_43.png"/> be a prior distribution over functions represented as a stochastic process, for example, a Gaussian process <img class="formulaInl" alt="$\mathbf{x}i(\cdot)$" src="form_44.png"/>, with inputs <img class="formulaInl" alt="$x \in \mathbb{X}$" src="form_45.png"/> and an associate kernel or covariance function <img class="formulaInl" alt="$k(\cdot,\cdot)$" src="form_46.png"/>. Let also assume that the target function is a sample of the stochastic process <img class="formulaInl" alt="$f \sim \mathbf{x}i(\cdot)$" src="form_47.png"/>.</p>
<p>In order to find the minimum, the algorithm has a maximum budget of <img class="formulaInl" alt="$N$" src="form_48.png"/> evaluations of the target function <img class="formulaInl" alt="$f$" src="form_28.png"/>. The purpose of the algorithm is to find optimal decisions that provide a better performance at the end.</p>
<p>One advantage of using Gaussian processes as a prior distributions over functions is that new observations of the target function <img class="formulaInl" alt="$(x_i,y_i)$" src="form_49.png"/> can be easily used to update the distribution over functions. Furthermore, the posterior distribution is also a Gaussian process <img class="formulaInl" alt="$\mathbf{x}i_i = \left[ \mathbf{x}i(\cdot) | x_{1:i},y_{1:i} \right]$" src="form_50.png"/>. Therefore, the posterior can be used as an informative prior for the next iteration in a recursive algorithm.</p>
<p>In a more general setting, many authors have suggested to modify the standard zero-mean Gaussian process for different variations that include semi-parametric models <a class="el" href="citelist.html#CITEREF_Huang06">[9]</a> <a class="el" href="citelist.html#CITEREF_Handcock1993">[7]</a> <a class="el" href="citelist.html#CITEREF_Jones:1998">[11]</a> <a class="el" href="citelist.html#CITEREF_OHagan1992">[19]</a>, use of hyperpriors on the parameters <a class="el" href="citelist.html#CITEREF_MartinezCantin09AR">[15]</a> <a class="el" href="citelist.html#CITEREF_Brochu:2010c">[1]</a> <a class="el" href="citelist.html#CITEREF_Hoffman2011">[8]</a>, Student t processes <a class="el" href="citelist.html#CITEREF_Gramacy_Polson_2009">[6]</a> <a class="el" href="citelist.html#CITEREF_Sacks89SS">[22]</a> <a class="el" href="citelist.html#CITEREF_Williams_Santner_Notz_2000">[27]</a>, etc.</p>
<p>We use a generalized linear model of the form: </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ f(x) = \phi(\mathbf{x})^T \mathbf{w} + \epsilon(\mathbf{x}) \]" src="form_51.png"/>
</p>
<p> where </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ \epsilon(\mathbf{x}) \sim \mathcal{NP} \left( 0, \sigma^2_s (\mathbf{K}(\theta) + \sigma^2_n \mathbf{I}) \right) \]" src="form_52.png"/>
</p>
<p> The term <img class="formulaInl" alt="$\mathcal{NP}$" src="form_53.png"/> means a non-parametric process, which can make reference to a Gaussian process <img class="formulaInl" alt="$\mathcal{GP}$" src="form_54.png"/> or a Student's t process <img class="formulaInl" alt="$\mathcal{TP}$" src="form_55.png"/>. In both cases, <img class="formulaInl" alt="$\sigma^2_n$" src="form_56.png"/> is the observation noise variance, sometimes called nugget, and it is problem specific. Many authors decide to fix this value <img class="formulaInl" alt="$\sigma^2_n = 0$" src="form_57.png"/> when the function <img class="formulaInl" alt="$f(x)$" src="form_58.png"/> is deterministic, for example, a computer simulation. However, as cleverly pointed out in <a class="el" href="citelist.html#CITEREF_Gramacy2012">[5]</a>, there might be more reasons to include this term appart from being the observation noise, for example, to consider model inaccuracies.</p>
<p>This model has been presented in different ways depending on the field where it was used: </p><ul>
<li>As a generalized linear model <img class="formulaInl" alt="$\phi(\mathbf{x})^T\mathbf{w}$" src="form_59.png"/> with heteroscedastic perturbation <img class="formulaInl" alt="$\epsilon(\mathbf{x})$" src="form_60.png"/>. </li>
<li>As a nonparametric process of the form <img class="formulaInl" alt="$\mathcal{NP} \left(\phi(\mathbf{x})^T\mathbf{w}, \sigma^2_s (\mathbf{K}(\theta) + \sigma^2_n \mathbf{I}) \right)$" src="form_61.png"/>. </li>
<li>As a semiparametric model <img class="formulaInl" alt="$f(\mathbf{x}) = f_{par}(\mathbf{x}) + f_{nonpar}(\mathbf{x}) = \phi(\mathbf{x})^T\mathbf{w} + \mathcal{NP}(\cdot)$" src="form_62.png"/></li>
</ul>
<h1><a class="anchor" id="modelopt"></a>
Models and functions</h1>
<p>This library was originally developed for as part of a robotics research project <a class="el" href="citelist.html#CITEREF_MartinezCantin09AR">[15]</a> <a class="el" href="citelist.html#CITEREF_MartinezCantin07RSS">[14]</a>, where a Gaussian process with hyperpriors on the mean and signal covariance parameters. Then, the metamodel was constructed using the Maximum a Posteriory (MAP) of the parameters. By that time, it only supported one kernel function, one mean function and one criterion.</p>
<p>However, the library now has grown to support many more surrogate models, with different distributions (Gaussian processes, Student's-t processes, etc.), with many kernels and mean functions. It also provides different criteria (even some combined criteria) so the library can be used to any problem involving some bounded optimization, stochastic bandits, active learning for regression, etc.</p>
<h2><a class="anchor" id="surrmod"></a>
Surrogate models</h2>
<p>As seen in Section modopt this library implements only one general regression model. However, we can assign a set of priors on the parameters of the model <img class="formulaInl" alt="$\mathbf{w}$" src="form_63.png"/>, <img class="formulaInl" alt="$\sigma_s^2$" src="form_64.png"/> (the kernel hyperparameter will be discussed in Section learnker). Thus, the options are:</p>
<ul>
<li>"sGaussianProcess": a standard Gaussian process where the hyperparameters are known. </li>
<li>"sGaussianProcessML": a standard Gaussian process where the hyperparameters are estimated directly from data using maximum likelihood estimates. </li>
<li>"sGaussianProcessNormal": a Gaussian process with a Normal prior on the mean function parameters <img class="formulaInl" alt="$\mathbf{w}$" src="form_63.png"/> and known <img class="formulaInl" alt="$\sigma_s^2$" src="form_64.png"/>. </li>
<li>"sStudentTProcessJef": in this case we use the Jeffreys prior for <img class="formulaInl" alt="$\mathbf{w}$" src="form_63.png"/> and <img class="formulaInl" alt="$\sigma_s^2$" src="form_64.png"/>. This is a kind of uninformative prior which is invariant to reparametrizations. Once we set a prior on <img class="formulaInl" alt="$\sigma_s^2$" src="form_64.png"/> the posterior becomes a Student's t Process. </li>
<li>"sStudentTProcessNIG": in this case we standard conjugate priors, that is, a Normal prior on <img class="formulaInl" alt="$\mathbf{w}$" src="form_63.png"/> and a Inverse Gamma on <img class="formulaInl" alt="$\sigma_s^2$" src="form_64.png"/>. Therefore, the posterior is again a Student's t process.</li>
</ul>
<p>Gaussian processes are a very general model that can achieve good performance with a reasonable computational cost. However, Student's t processes, thanks to the hierarchical structure of priors, provide an even more general setup for a minor extra cost. Furthermore, the Student's t distribution is robust to outliers and heavy tails in the data.</p>
<h2><a class="anchor" id="kermod"></a>
Kernel (covariance) models</h2>
<p>One of the critical components of Gaussian and Student's t processes is the definition of the kernel function, which defines the correlation between points in the input space. As a correlation function, the kernel must satisfy a set of properties (e.g.: being positive definite). All the kernel models available and its combinations satisfy the kernel restrictions.</p>
<p>The functions with <b>"ISO"</b> in their name are <em>isotropic</em> function, that is, they share a single set of parameters for all the dimensions of the input space.</p>
<p>The functions with <b>"ARD"</b> in their name use <em>Automatic Relevance Determination</em>, that is, they use independent parameters for every dimension of the problem. Therefore, they can be use to find the <em>relevance</em> of the each feature in the input space. In the limit, this can be used for feature selection.</p>
<h3><a class="anchor" id="singker"></a>
Atomic kernels</h3>
<ul>
<li>"kConst": a simple constant function. </li>
<li>"kLinear", "kLinearARD": a linear function. </li>
<li>"kMaternISO1", "kMaternISO3","kMaternISO5","kMaternARD1","kMaternARD3","kMaternARD5": Matern kernel functions. The number divided by 2 represents the order of the function. See <a class="el" href="citelist.html#CITEREF_Rasmussen:2006">[20]</a> for a description. </li>
<li>"kPolyX": Polynomial kernel function. X is a number 1-6 which represents the exponent of the function. </li>
<li>"kSEARD","kSEISO": Squared exponential kernel, also known as Gaussian kernel. </li>
<li>"kRQISO": Rational quadratic kernel, also known as Student's t kernel.</li>
</ul>
<h3><a class="anchor" id="combker"></a>
Binary kernels</h3>
<p>This kernels allow to combine some of the previous kernels. </p><ul>
<li>"kSum": Sum of kernels. </li>
<li>"kProd": Product of kernels.</li>
</ul>
<p>Note that the binary kernels only admits two terms. However, we can combine them for more complex operations. For example if we write:</p>
<p>"kSum(kMaternISO3,kSum(kRQISO,kProd(kPoly4,kConst))"</p>
<p>it represents the expresion: Matern(3) + RationalQuadratic + C*Polynomial^4</p>
<p>In this case, the vector of parameters is splited from left to right: 1 for the Matern function, 2 for the RQ function, 2 for polynomial function and 1 for the constant. If the vector of parameters have more or less than 6 elements, the system complains.</p>
<h2><a class="anchor" id="parmod"></a>
Parametric (mean) functions</h2>
<p>Although the nonparametric process is able to model a large amount of funtions, we can model the expected value of the nonparametric process as a parametric function. This parametric model will help to capture large offsets and global trends.</p>
<p>The usage is analogous to the kernel functions.</p>
<ul>
<li>"mZero","mOne","mConst": constant functions. For simplicity and because they are largely used, we provide special cases f(x) = 0 and f(x) = 1. </li>
<li>"mLinear": linear function. </li>
<li>"mSum": binary function which can be used to combine other functions.</li>
</ul>
<h2><a class="anchor" id="critmod"></a>
Selection criteria</h2>
<p>As discussed in <a class="el" href="bopttheory.html#introbopt">Introduction to Bayesian Optimization</a>, one of the critical aspects for Bayesian optimization is the decision (loss) function. Unfortunately, the functions described there are unavailable, because they assume knowledge of the optimal value <img class="formulaInl" alt="$x^*$" src="form_15.png"/>. However, we can define proxies for those functions.</p>
<p>Some criteria, such as the expected improvement and the lower confidence bound admits an annealed version "cXXXa". In that version, the parameter that is used to trade off exploration and exploitation changes over time to priorize exploration at the begining and exploitation at the end.</p>
<p>Many criteria depends on the prediction function, which can be a Gaussian or a Student's t distribution, depending on the surrogate model. However, the library includes all the criteria for both distributions, and the system automatically selected the correct one.</p>
<h3><a class="anchor" id="atomcri"></a>
Atomic criteria</h3>
<ul>
<li>"cEI","cBEI","cEIa": The most extended and reliable algorithm is the Expected Improvement algorithm <a class="el" href="citelist.html#CITEREF_Mockus78">[16]</a>. In this case we provide the general version from <a class="el" href="citelist.html#CITEREF_Schonlau98">[23]</a> which includes an exponent to trade off exploration and exploitation "cEI". For an annealed version of the exploration/exploitation trade off, use "cEIa". Whe also includes a variation from <a class="el" href="citelist.html#CITEREF_Mockus1989">[17]</a> which add a <em>bias</em> or <em>threshold</em> to the improvement "cBEI". </li>
<li>"cLCB", "cLCBa": Another popular algorithm is the Lower Confidence Bound (LCB), or UCB in case of maximization. Introduced by <a class="el" href="citelist.html#CITEREF_cox1992statistical">[4]</a> as Sequential Design for Optimization (SDO). Analogously, "cLCBa" represents an annealed version of the exploration/exploitation trade off. </li>
<li>"cMI": A generalized version of the LCB criterion which relies on the mutual information. See <a class="el" href="citelist.html#CITEREF_Contal2014">[3]</a> </li>
<li>"cPOI": Probability of improvement, by <a class="el" href="citelist.html#CITEREF_Kushner:1964">[12]</a> </li>
<li>"cExpReturn","cThompsonSampling","cOptimisticSampling": This criteria are related with the predicted return of the function. The first one is literally the expected return of the function (mean value). The second one is based on the Thompson sampling (drawing a random sample from the predicted distribution). Finally, the optimistic sampling takes the minimum of the other two (mean vs random). </li>
<li>"cAopt": This is based on the A-optimality criteria. It is the predicted variance at the query point. Thus, this criteria is intended for <b>exploration</b> of the input space, not for optimization. </li>
<li>"cDistance": This criteria adds a cost to a query point based on the distance with respect to the previous evaluation. Combined with other criteria functions, it might provide a more realistic setup for certain applications <a class="el" href="citelist.html#CITEREF_Marchant2012">[13]</a></li>
</ul>
<h3><a class="anchor" id="combcri"></a>
Combined criteria</h3>
<ul>
<li>"cSum","cProd": Sum and product of different criteria functions. </li>
<li>"cHedge", "cHedgeRandom": Bandit based selection of the best criteria based on the GP-Hedge algorithm <a class="el" href="citelist.html#CITEREF_Hoffman2011">[8]</a>. It automatically learns based on the behaviour of the criteria during the optimization process. The original version "cHedge" uses the maximum expected return as a <em>reward</em> for each criteria. We add a variant "cHedgeRandom" where the <em>reward</em> is defined in terms of Thompson sampling.</li>
</ul>
<p>In this case, the combined criteria admits more that two functions. For example:</p>
<p>"cHedge(cSum(cEI,cDistance),cLCB,cPOI,cOptimisticSampling)"</p>
<h2><a class="anchor" id="learnmod"></a>
Methods for learning the kernel parameters</h2>
<p>The posterior distribution of the model, which is necessary to compute the criterion function, cannot be computed in closed form if the kernel hyperparameters are unknown. Thus, we need a find to approximate this posterior distribution conditional on the kernel hyperparameters.</p>
<p>First, we need to consider if we are going to use a full Bayesian approach or an empirical Bayesian approach. The first one, computes the full posterior distribution by propagation of the uncertainty of each element and hyperparameter to the posterior. In this case, it can be done by discretization of the hyperparameter space or by using MCMC (not yet implemented). In theory, it is more precise but the computation burden can be orders of magnitude higher. The empirical approach on the other hand computes a point estimate of the hyperparameters based on some score function and use it as a "true" value. Although the uncertainty estimation in this case might not be as accurate as the full Bayesian, the computation overhead is minimal.</p>
<p>For the score function, we need to find the likelihood function of the observed data for the parameters. Depending on the model, this function will be a multivariate Gaussian distribution or multivariate t distribution. In general, we present the likelihood as a log-likelihood function up to a constant factor, that is, we remove the terms independent of <img class="formulaInl" alt="$\theta$" src="form_65.png"/> from the log likelihood. In practice, whether we use a point estimate (maximum score) or full Bayes MCMC/discrete posterior, the constant factor is not needed.</p>
<p>We are going to consider the following score functions to learn the kernel hyperparameters:</p>
<ul>
<li>Leave one out cross-validation (SC_LOOCV): In this case, we try to maximize the average predicted log probability by the <em>leave one out</em> (LOO) cross-validation strategy. This is sometimes called a pseudo-likelihood.</li>
</ul>
<ul>
<li>Maximum Total Likelihood (SC_MTL) For any of the models presented, one approach to learn the hyperparameters is to maximize the likelihood of all the parameters <img class="formulaInl" alt="$\mathbf{w}$" src="form_63.png"/>, <img class="formulaInl" alt="$\sigma_s^2$" src="form_64.png"/> and <img class="formulaInl" alt="$\theta$" src="form_65.png"/>. Then, the likelihood function is a multivariate Gaussian distribution. We can obtain a better estimate if we adjust the number of degrees of freedom, this is called <em>restricted maximum likelihood</em>. The library automatically selects the restricted version, if it is suitable.</li>
</ul>
<ul>
<li>Posterior maximum likelihood (SC_ML): In this case, the likelihood function is modified to consider the posterior estimate of <img class="formulaInl" alt="$(\mathbf{w},\sigma_s^2)$" src="form_66.png"/> based on the different cases defined in Section surrmods. In this case, the function will be a multivariate Gaussian or t distribution, depending on the kind of prior used for <img class="formulaInl" alt="$\sigma_s^2$" src="form_64.png"/>.</li>
</ul>
<ul>
<li>Maximum a posteriori (SC_MAP): We can modify the previous algorithms by adding a prior distribution <img class="formulaInl" alt="$p(\theta)$" src="form_67.png"/>. By default, we add a joint normal prior on all the kernel hyperparameters. However, if the variance of the prior <em>hp_std</em> is invalid (&lt;=0), then we assume a flat prior on that hyperparameter. Since we assume that the hyperparameters are independent, we can apply priors selectively only to a small set.</li>
</ul>
<h2><a class="anchor" id="initdes"></a>
Initial design methods</h2>
<p>In order to build a suitable surrogate function, we a need a preliminar set of samples. In Bayesian optimization this is typically performed using alternative experimental design criteria. In this first step, usually the main criteria is space filling. Thus, we have implemented the subsequent designs:</p>
<ul>
<li>Latin hypercube sampling: Each dimension of the space is divided in several intervals. Samples are then taken according to a generalization of the Latin square scheme. <a href="http://en.wikipedia.org/wiki/Latin_hypercube_sampling">http://en.wikipedia.org/wiki/Latin_hypercube_sampling</a></li>
</ul>
<ul>
<li>Sobol sequences: It is a set of quasi-random low-discrepancy sequences. Thus the space is sampled more evenly than with uniform sampling. <a href="http://en.wikipedia.org/wiki/Sobol_sequence">http://en.wikipedia.org/wiki/Sobol_sequence</a></li>
</ul>
<ul>
<li>Uniform sampling: The search space is sampled uniformly.</li>
</ul>
<p>Note: Since we do not assume any struture in the set of discrete points during discrete optimization, only uniform sampling of the discrete set is available in that case. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="reference.html">Reference Manual</a></li>
    <li class="footer">Generated on Wed Feb 25 2015 01:47:51 for BayesOpt by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.9.1 </li>
  </ul>
</div>
</body>
</html>
